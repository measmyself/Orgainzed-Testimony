# ğŸ“ Feedback Metrics Automation â€“ Legal Aid Society (v2.2.3)

A Python-based automation system for transforming raw LMS course feedback into clean, summarized reports â€” no pivot tables required. Developed for repeatable, low-touch reporting at the Legal Aid Society.

---

## âœ… What It Does

1. **Ingests Raw Feedback**
   - Accepts `.xls` exports from viDesktop
   - Converts text ratings (e.g. Excellent, Good) into a numeric 4-point scale
   - Extracts open-ended comment responses
   - Removes exact and partial duplicates

2. **Processes Cleaned Data**
   - Cleans malformed headers (e.g., `_x000D_`)
   - Dynamically detects key columns
   - Validates all responses use the 4-point scale
   - Computes summary metrics per question

3. **Exports Results**
   - Outputs Excel + CSV reports to `/Feedback_Summary`
   - Auto-adjusts Excel columns
   - Generates logs for each run

---

## ğŸ“ Folder Structure

```
2025 Organized Testimony/
â”œâ”€â”€ raw_evals/                    â† Drop raw .xls files here  
â”œâ”€â”€ processed_reports/            â† Cleaned .xlsx output files  
â”œâ”€â”€ Feedback_Summary/             â† Summary metrics + comments  
â”œâ”€â”€ logs/                         â† Execution logs  
â”œâ”€â”€ feedback_ingestion.py         â† Preprocesses and deduplicates feedback  
â”œâ”€â”€ feedback_metrics.py           â† Analyzes and summarizes data  
â”œâ”€â”€ Organized_requirementsutils.py â† Shared helper functions (e.g., file cleanup)  
```

---

## ğŸš€ How to Use

### STEP 1: Clean Raw Files
```bash
python feedback_ingestion.py
```
- Converts `.xls` â†’ `.xlsx`
- Maps text ratings to numbers (1â€“4)
- Removes duplicate responses
- Extracts comments

### STEP 2: Generate Feedback Report
```bash
python feedback_metrics.py
```
- Summarizes ratings per question
- Outputs:
  - `Feedback_Summary_<timestamp>.xlsx`
  - `Feedback_Summary_<timestamp>.csv`

---

## ğŸ§¼ Cleanup Prompt

Before analysis, you'll be prompted to clean out `/Feedback_Summary` files:

- `y` â€“ delete all matching files  
- `n` â€“ skip deletion and continue  
- `dry-run` â€“ show what would be deleted  
- `p` â€“ preview all files

---

## ğŸ“Š Summary Output â€“ Column Definitions

| Column         | Description |
|----------------|-------------|
| `event`        | Name of the training or session (from file name or metadata) |
| `question`     | The feedback item being rated |
| `average`      | Mean rating across all valid responses |
| `median`       | Middle value of response scores |
| `std_dev`      | Standard deviation â€“ how spread out the responses are. A higher number means more variation. |
| `min`          | The lowest response score received. |
| `max`          | The highest response score received. |
| `count`        | Total number of responses collected for that question. |
| `positive_pct` | Percentage of responses that were rated 4 or higher on the scale (e.g., â€œGoodâ€ or â€œExcellentâ€). |

---

## ğŸ§ª Version Info

- **Current Version**: `v2.2.3`
- **Rating Scale**: 4-point (Excellent = 4, Good = 3, Fair = 2, Poor = 1)
- **Maintainer**: Geno Moore, Legal Aid Society

---

## ğŸ“Œ Notes

- Make sure `feedback_ingestion.py` is run before generating summaries
- All column names are cleaned and normalized
- Safe deduplication logic prevents inflated metrics
